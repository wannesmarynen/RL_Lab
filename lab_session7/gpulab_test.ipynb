{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "colab": {
   "name": "test.ipynb",
   "provenance": []
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGqnGCpksLQz",
    "colab_type": "text"
   },
   "source": [
    "# Lab session 7\n",
    "\n",
    "In this notebook, we'll be building a vanilla DQN, a DQN with experience replay, a DQN with target networks. \n",
    "### What is DQN?\n",
    "\n",
    "DQN is the first deep reinforcement learning method proposed by DeepMind, it overcomes unstable learning by mainly 4 techniques:\n",
    ">* Experience Replay.\n",
    "* Target Network.\n",
    "* Clipping Rewards.\n",
    "* Skipping Frames. \n",
    "\n",
    "To keep this lab session in time, instead of using gym atari we'll work with classic cart pole environment.\n",
    "<img src='assets/cartpole.gif' width=50% />\n",
    "\n",
    "#### Note:-\n",
    "Apart from submitting code, please add your answer to questions mentioned in lab session.\n",
    "Operation on cuda are fast. Try keeping all your operations on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5ln45VPsLQ3",
    "colab_type": "text"
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jh8kyVltsLQ7",
    "colab_type": "text"
   },
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nXrO1S-4sLQ9",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "import math, random\n",
    "\n",
    "import cv2\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nj00nCxSsLRN",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KOjh-ZDisLRf",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWC1ntdosLRp",
    "colab_type": "text"
   },
   "source": [
    "**Note** *test()* function is a function to watch your agent play the game after training. Call the following function by passing your trained model and environment name as parameter.\n",
    "\n",
    "It returns a list of rewards."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DJkdSXHWsLRr",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "def test(env_id, model):\n",
    "    \"\"\"\n",
    "    Method to play 30 games with trained agent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env_id: str\n",
    "        name of your environment\n",
    "    model: model\n",
    "    \n",
    "    Returns\n",
    "    all_test_rewards: list\n",
    "        list of rewards collected in 30 episodes.\n",
    "    \"\"\"\n",
    "    env = gym.make(env_id)\n",
    "    all_test_rewards = list()\n",
    "    episode_reward = 0\n",
    "    episode_count = 0\n",
    "    state = env.reset()\n",
    "    plt.figure(figsize=(9,9))\n",
    "    img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "    while True:\n",
    "        if episode_count == 30:\n",
    "            break\n",
    "        img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        action = model.act(state,-1) # -ve epsilon so it always choses the policy it learned rather than random action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if not done:\n",
    "            episode_reward += reward\n",
    "        else:\n",
    "            state = env.reset()\n",
    "            all_test_rewards.append(episode_reward)\n",
    "            print(episode_reward)\n",
    "            episode_reward = 0\n",
    "            episode_count += 1\n",
    "    print(\"The mean score of your agent: \", np.mean(all_test_rewards))\n",
    "    env.close()\n",
    "    return all_test_rewards"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OS5QZWCDsLR1",
    "colab_type": "text"
   },
   "source": [
    "#### Set cuda"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ctEbKR3_sLR2",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QreopiqusLR_",
    "colab_type": "text"
   },
   "source": [
    "#### Cart Pole Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hfQdbkwGsLSB",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7RPE34bsLSM",
    "colab_type": "text"
   },
   "source": [
    "#### Epsilon greedy exploration\n",
    "I'm sure you know the importance of epsilon. \n",
    "we are gonna plot the decay and you can see how it will behave for 10000 steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kCp__pX4sLSO",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8We9mg0LsLSV",
    "colab_type": "code",
    "outputId": "038862a3-2b6d-4b75-8304-ec9c8d51759e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "plt.plot([epsilon_by_frame(i) for i in range(10000)])"
   ],
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7fd5e831e760>]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 23
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJlyPWCZsLSj",
    "colab_type": "text"
   },
   "source": [
    "### Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EmeIaWRAsLSl",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_actions = num_actions\n",
    "        self.fc1 = nn.Linear(num_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, self.num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state   = Variable(torch.FloatTensor(state).unsqueeze(0))\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0].cpu().numpy().tolist()\n",
    "        else:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        return action"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HnrOd5OGsLSw",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "model_base = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model_base = model_base.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model_base.parameters())"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5ReYOClsLTD",
    "colab_type": "text"
   },
   "source": [
    "#### Computing Temporal Difference Loss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OIWe4vfOsLTE",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "def compute_td_loss_dqn(state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    state: numpy array\n",
    "        current state of env\n",
    "    action: int\n",
    "        action you take in env\n",
    "    reward: int\n",
    "        reward for the action\n",
    "    next_state: numpy array\n",
    "        next state given current state and action\n",
    "    done: bool\n",
    "        if it's end of the episode.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss: 1d-tensor\n",
    "\n",
    "    \"\"\"\n",
    "    # converting variables to tensors\n",
    "    state = Variable(torch.FloatTensor(np.float32(state)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    \n",
    "    action = Variable(torch.from_numpy(np.array(action)))\n",
    "\n",
    "    reward = Variable(torch.from_numpy(np.array(reward)))\n",
    "    \n",
    "    done = Variable(torch.from_numpy(np.array(int(done))))\n",
    "    \n",
    "    # TODO:- predict q_values\n",
    "    q_values = model_base.forward(state)\n",
    "    \n",
    "    # TODO:- predict q_values of next state\n",
    "    next_q_values = model_base.forward(next_state)\n",
    "    \n",
    "    # TODO:- get corresponding action's q-value from predicted q-values\n",
    "    # hint:- there is a very optimal and good way to do this tensor operations check (gather)\n",
    "    q_value = torch.gather(q_values,0,action)    \n",
    "    \n",
    "    next_q_value = next_q_values.max(0)[0]\n",
    "    \n",
    "    # TODO:- calculate expected q value based on bellman eq.\n",
    "    expected_q_value = reward + gamma * next_q_value if not done else reward\n",
    "    \n",
    "    # TODO:- calculate MSE for back propagation of loss\n",
    "    # Hint:- you can't do an operation between a tensor and int. keep an eye on your variable type ;).\n",
    "    loss = F.mse_loss(expected_q_value , q_value)\n",
    "    \n",
    "    # back propagating your loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "II3YFY1csLTQ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame_idx: int\n",
    "        frame id\n",
    "    rewards: int\n",
    "        accumulated reward\n",
    "    losses: int\n",
    "        loss\n",
    "    \"\"\"\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GedJCkAEsLTX",
    "colab_type": "code",
    "outputId": "e3e59612-a715-4aef-ad5d-44ed1ffed77d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "num_frames = 20000\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model_base.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    \n",
    "    loss = compute_td_loss_dqn(state, action, reward, next_state, done)\n",
    "    losses.append(loss.item())\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1440x360 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJo-8GOgsLTf",
    "colab_type": "text"
   },
   "source": [
    "### Deep Q-Network with Replay buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLh3kVOAsLTh",
    "colab_type": "text"
   },
   "source": [
    "#### Experience Replay\n",
    "\n",
    "Experience replay will help us to handle two things:\n",
    "\n",
    "Avoid forgetting previous experiences.\n",
    "Reduce correlations between experiences.\n",
    "\n",
    "> * **Avoid forgetting previous experiences** : With using same network to generise we run into a problem. Since the environment is sequential we see new state, take action and overwrite the weights of our previous experience. Since all the weights are shared they form a high correlation between actions and states. We can greatly stablise our problem if we keep our previous experiences and learn from them as well. In environments like atari where your player dies and start episode again, training on these previous experiences come in handy as now agent still remember from previous experiences.\n",
    "> * **Reduce correlations between experiences**: We have another problem  we know that every action affects the next state. This outputs a sequence of experience tuples which can be highly correlated.\n",
    "If we train the network in sequential order, we risk our agent being influenced by the effect of this correlation.\n",
    "By sampling from the replay buffer at random, we can break this correlation. This prevents action values from oscillating or diverging catastrophically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TIQVMpGAsLTi",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "model_batch = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model_batch = model_batch.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model_batch.parameters())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNAi8VmysLTr",
    "colab_type": "text"
   },
   "source": [
    "#### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nAWdNXgnsLTw",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity: int\n",
    "            the length of your buffer\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        batch_size: int\n",
    "        \"\"\"\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4zq_anbsLT5",
    "colab_type": "text"
   },
   "source": [
    "#### Compute TD loss\n",
    "Reason we are redefining it is because with replay buffer now you are gonna save your experience, gather it from your replay buffer and train model on it again. \n",
    "<br>\n",
    "Now since it's a batch operation you have added an extra axis and you'll have operations in different axis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3QYnGY_KsLT6",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def compute_td_loss_batch(batch_size):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size: int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss: tensor\n",
    "    \"\"\"\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    with torch.no_grad():\n",
    "        next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "    \n",
    "    # TODO: predict q_values. \n",
    "    # Hint:- remeber it's a batch prediction so there will be extra dimention.\n",
    "    q_values = model_base.forward(state)\n",
    "    \n",
    "    # TODO: predict next state's q_values.\n",
    "    next_q_values = model_base.forward(state)\n",
    "\n",
    "    # TODO: get the q_values based on actions you took.\n",
    "    #Hint:- the logic should be same as previous one but remember there is an extra dimention.\n",
    "    print(q_values, action)\n",
    "    q_value = torch.gather(q_values,1,action)\n",
    "    \n",
    "    next_q_value = next_q_values.max(1)[0]\n",
    "    \n",
    "    # TODO: calculate expected q value based on bellman eq.\n",
    "    expected_q_value = ''\n",
    "    \n",
    "    # TODO: calculate MSE\n",
    "    loss = ''\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qcjlbBTsLUE",
    "colab_type": "text"
   },
   "source": [
    "As mentioned previously, DQN itself is not stable but we stablise it by multiple methods. Replay buffer is one of them. <br>\n",
    "If your performance of DQN with replay buffer is same as previous, Try running it couple of times"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WyHzgdw_sLUG",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 964
    },
    "outputId": "aac65b5e-dcaf-421b-b92d-3038a7d000c4",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "num_frames = 20000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "count = 0\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model_batch.act(state, epsilon)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        loss = compute_td_loss_batch(batch_size)\n",
    "        loss = loss.data.numpy().tolist()\n",
    "        losses.append(loss)\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        count = plot(frame_idx, all_rewards, losses)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3n2E_hCsLUM",
    "colab_type": "text"
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbIY7JqjsLUP",
    "colab_type": "text"
   },
   "source": [
    "### DQN with target network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO4C9q0fsLUR",
    "colab_type": "text"
   },
   "source": [
    "We saw in the *Deep Q Learning* that, when we want to calculate the TD error (aka the loss), we calculate the difference between the TD target (Q_target) and the current Q value (estimation of Q). <br>\n",
    "\n",
    "But **we dont have any idea of the real TD target**. We need to estimate it. Using the **Bellman equation**, we saw that the TD target is just the reward of taking that action at that state plus the discounted highest Q value for the next state. <br>\n",
    "\n",
    "However, the problem is that we using the same parameters (weights) for estimating the target and the Q value. As a consequence, there is a big correlation between the TD target and the parameters (w) we are changing.\n",
    "\n",
    "Therefore, it means that at every step of training, our Q values shift but also the target value shifts. So, were getting closer to our target but the target is also moving. Its like chasing a moving target! This lead to a big oscillation in training.\n",
    "\n",
    "To avoid this oscillation instead, we can use the idea of fixed Q-targets:\n",
    "\n",
    "> * Using a separate network with a fixed parameter (we'll call it Target network) for estimating the TD target. <br>\n",
    "while we train our DQN network.\n",
    "> * After some number of steps, we copy the parameters from our DQN network to update the target network.\n",
    "\n",
    "Well have more stable learning because the target function stays fixed for a while.\n",
    "\n",
    "In current implementation you won't be using Replay buffer, just *Target network*."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ecgOkkj1sLUU",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "current_model_target = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "target_model_target  = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model_target = current_model_target.cuda()\n",
    "    target_model_target  = target_model_target.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(current_model_target.parameters())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trsSLQ0WsLUf",
    "colab_type": "text"
   },
   "source": [
    "#### Sync model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yaSGcxEMsLUh",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def update_target(current_model, target_model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_model: model\n",
    "    \n",
    "    target_model: model\n",
    "    \"\"\"\n",
    "    target_model.load_state_dict(current_model.state_dict())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uDEKgBuVsLUp",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# initialise network as same as they may have random values in their weights\n",
    "update_target(current_model_target, target_model_target)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_SgFm5-sLUv",
    "colab_type": "text"
   },
   "source": [
    "#### Compute TD loss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N0s8-wyUsLUx",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def compute_td_loss_target(state, action, reward, next_state, done):\n",
    "    \n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    with torch.no_grad():\n",
    "        next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action  = Variable(torch.from_numpy(np.array(action)))\n",
    "    reward  = Variable(torch.from_numpy(np.array(reward)))\n",
    "    done = Variable(torch.from_numpy(np.array(int(done))))\n",
    "    \n",
    "    #TODO:- predict q_values.\n",
    "    q_values = \n",
    "    \n",
    "    # TODO:- predict next states's q_values based on target.\n",
    "    next_q_values = \n",
    "    \n",
    "    # TODO:- Get q-values for actions agent took.\n",
    "    q_value = \n",
    "    \n",
    "    # TODO:- gather next q_values of target model.\n",
    "    # Hint:- next_q_values = next_q_state_values[maximum of next_q_values]\n",
    "    next_q_value = \n",
    "    \n",
    "    # TODO:- calculate expected q value based on bellman eq.\n",
    "    expected_q_value = \n",
    "\n",
    "    # TODO:- MSE\n",
    "    loss = \n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "udGjk7phsLU-",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "num_frames = 20000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = current_model_target.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    loss = compute_td_loss_target(state, action, reward, next_state, done)\n",
    "    losses.append(loss.data)\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)\n",
    "        \n",
    "    if frame_idx % 100 == 0:\n",
    "        update_target(current_model_target, target_model_target)\n",
    "        "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns1OymZVsLVF",
    "colab_type": "text"
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4TJzD_AsLVH",
    "colab_type": "text"
   },
   "source": [
    "### DQN with target network and Replay buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ake4dpTtsLVJ",
    "colab_type": "text"
   },
   "source": [
    "Let's try adding Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sdH7aqL0sLVK",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "current_model_dqn = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "target_model_dqn  = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model_dqn = current_model_dqn.cuda()\n",
    "    target_model_dqn  = target_model_dqn.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(current_model_dqn.parameters())\n",
    "update_target(current_model_dqn, target_model_dqn)\n",
    "replay_buffer = ReplayBuffer(1000)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5qK68J8sLVS",
    "colab_type": "text"
   },
   "source": [
    "#### Compute TD loss"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "spscvgYysLVU",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def compute_td_loss_target_batch(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    # Hint:- most of the operations should be similar to previous impletations.\n",
    "    \n",
    "    #TODO:- predict q_values.\n",
    "    q_values = \n",
    "    \n",
    "    # TODO:- predict next states's q_values based on target.\n",
    "    next_q_values = \n",
    "    \n",
    "    # TODO:- Get q-values for actions agent took.\n",
    "    q_value = \n",
    "    \n",
    "    # TODO:- gather maximum of next q_values  \n",
    "    next_q_value = \n",
    "    \n",
    "    # TODO:- calculate expected q value based on bellman eq.\n",
    "    expected_q_value = \n",
    "      \n",
    "    # TODO MSE\n",
    "    loss = \n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-8XrW3gpsLVc",
    "colab_type": "code",
    "outputId": "9add27c3-5cdd-4c79-dc42-22e04ed755ba",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "num_frames = 20000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = current_model_dqn.act(state, epsilon)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        loss = compute_td_loss_target_batch(batch_size)\n",
    "        losses.append(loss.data)\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)\n",
    "        \n",
    "    if frame_idx % 100 == 0:\n",
    "        update_target(current_model_dqn, target_model_dqn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VkPPweJsLVi",
    "colab_type": "text"
   },
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdkbPnpJsLVj",
    "colab_type": "text"
   },
   "source": [
    "### Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hU98OvXCsLVm",
    "colab_type": "text"
   },
   "source": [
    "The target based DQN in early stages is naive, we use the action that maximizes the Q-value[next_state]. But in the early stage, this is a noisy approximation so we tend to overestimate the Q-value.\n",
    "\n",
    "To overcome the overestimation problem we can use both the networks the local and target as we have two sets of weights, so we can cross-validate it with both sets of weights and minimize the overestimation problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OVGey8CTsLVp",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "current_model_ddqn = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "target_model_ddqn  = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model_ddqn = current_model_ddqn.cuda()\n",
    "    target_model_ddqn  = target_model_ddqn.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(current_model_ddqn.parameters())\n",
    "update_target(current_model_ddqn, target_model_ddqn)\n",
    "replay_buffer = ReplayBuffer(1000)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hUFDNMrrsLVu",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def compute_td_loss_doubleDQN(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    with torch.no_grad():\n",
    "        next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action  = Variable(torch.from_numpy(np.array(action)))\n",
    "    reward     = Variable(torch.from_numpy(np.array(reward)))\n",
    "    done       = Variable(torch.from_numpy(np.array(int(done))))\n",
    "    \n",
    "    # TODO:- predict current model\n",
    "    q_values = \n",
    "    \n",
    "    # TODO:- predict next value from current model\n",
    "    next_q_values = \n",
    "    \n",
    "    # TODO:- predict next state values from target model\n",
    "    next_q_target_values = \n",
    "    \n",
    "    # Gather only values based on action\n",
    "    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # TODO:- calculate next state q_values that are not over estimated\n",
    "    # Hint:- next_q_value = next state q-values from target[maximum of next state q-values from current model]\n",
    "    next_q_value = \n",
    "    \n",
    "    # TODO:- calculate expected Q-values of future\n",
    "    expected_q_value = \n",
    "      \n",
    "    \n",
    "    # TODO:- apply MSE\n",
    "    loss = \n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MSDlVhDnsLVy",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "num_frames = 20000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = current_model_ddqn.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    loss = compute_td_loss_doubleDQN(batch_size)\n",
    "    losses.append(loss.data)\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)\n",
    "        \n",
    "    if frame_idx % 100 == 0:\n",
    "        update_target(current_model_ddqn, target_model_ddqn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0eGrp45sLV5",
    "colab_type": "text"
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pjbGzB8sLV6",
    "colab_type": "text"
   },
   "source": [
    "### Dueling Deep Q Networks\n",
    "The key motivation behind this Dueling-DQN is that, for some problem statements, it is unnecessary to know the value of each action at every timestep. \n",
    "A simple example is of the Atari game Enduro, where it is not necessary to know which action to take until collision is imminent.\n",
    "\n",
    "<img src='assets/enduro.gif' width=50% />\n",
    "\n",
    "##### Architecture\n",
    "Like the standard DQN architecture, we have layers to process data. From there, we split the network into two separate streams, one for estimating the state-value and the other for estimating state-dependent action advantages. After the two streams, the last module of the network combines the state-value and advantage outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LKZi6UunsLV7",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.num_actions = num_outputs\n",
    "        \n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_outputs)\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        advantage = self.advantage(x)\n",
    "        value     = self.value(x)\n",
    "        return value + advantage  - advantage.mean()\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state   = Variable(torch.FloatTensor(state).unsqueeze(0))\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0].numpy().tolist()\n",
    "        else:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        return action"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3Av8jJq8sLV_",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "current_model_dueldqn = DuelingDQN(env.observation_space.shape[0], env.action_space.n)\n",
    "target_model_dueldqn  = DuelingDQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model_dueldqn = current_model_dueldqn.cuda()\n",
    "    target_model_dueldqn  = target_model_dueldqn.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(current_model_dueldqn.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNWqC70osLWG",
    "colab_type": "text"
   },
   "source": [
    "#### Synchronize current policy net and target net"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7uJNWz1ksLWH",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "update_target(current_model_dueldqn, target_model_dueldqn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZC5JvAVmsLWO",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def compute_td_loss_duelingDQN(batch_size):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "    \n",
    "    # TODO: Predict q_values from current model\n",
    "    q_values = \n",
    "    \n",
    "    # TODO: Predict q_values from target model\n",
    "    next_q_values = \n",
    "    \n",
    "    # TODO: get q_values based on action from q_values\n",
    "    q_value  = \n",
    "    \n",
    "    # TODO: find maximum of next_q_values \n",
    "    # Hint: keep in mind its a batch process so find maximum in correct axis\n",
    "    next_q_value =\n",
    "    \n",
    "    # TODO: calculate expected_q_value derived from bellman eq.\n",
    "    expected_q_value = \n",
    "    \n",
    "    # TODO: calculate MSE between q_value and expected_q_value\n",
    "    loss = \n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hl3u9ERssLWZ",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "num_frames = 20000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = current_model_dueldqn.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > batch_size:\n",
    "        loss = compute_td_loss_duelingDQN(batch_size)\n",
    "        losses.append(loss.data)\n",
    "        \n",
    "    if frame_idx % 200 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)\n",
    "        \n",
    "    if frame_idx % 100 == 0:\n",
    "        update_target(current_model_dueldqn, target_model_dueldqn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbJiL4oWsLWh",
    "colab_type": "text"
   },
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1u1xePhsLWi",
    "colab_type": "text"
   },
   "source": [
    "# Plot comparison of all agents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Cz4TuvDBsLWj",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "\"\"\"\n",
    "TODO: write a function to plot the performance of all previously discussed agents.\n",
    "Use test function to get their perform.\n",
    "\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8wDjwsUsLWs",
    "colab_type": "text"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL0tI-oCsLWt",
    "colab_type": "text"
   },
   "source": [
    "## Playing atari Pong with Double DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACqwqaM-sLWu",
    "colab_type": "text"
   },
   "source": [
    "You have successfully completed the DQN, Now we'll step up the complexity of our **Environment**. <br>\n",
    "With Complex visual environment like *Space invaders*, We'll need to include following steps:\n",
    "* Pre-process the input\n",
    "* Make our NN more complex\n",
    "> * Pre-process the input: To reduce the complexity of our states, to reduce the computation time needed for training and thus to keep network shallow we pre-process each state(frame) to convert it from RGB to grey scale. By doing this we reduce the 3 color channels (RGB) to 1 Grey. \n",
    "then we crop the top of frame where we see the score for 2 reasons\n",
    "    * Score doesn't matter as we are getting reward from env, thus it's just something we don't need.\n",
    "    * Score keeps on changing which introduces extra complexity adds more number of states. A small change in pixels potentially means new state\n",
    "> * Make our NN more complex: We introduce CNNs to preprocess the information from states(frames). You can further read on internet the advantages of CNNs over Fully connected layer. It's out of scope for this lab session.\n",
    "\n",
    "The new architecture will look something like this:-\n",
    "<img src='assets/atari.png' width=50% />\n",
    "\n",
    "This may seem complex but it's really not. we'll walk through step by step.\n",
    "\n",
    "1) Preprocess frame\n",
    "\n",
    "2) stack 4 frames: We stack frame to limit the problem of temporal limitation. \n",
    "  >* Temporal limitation: By looking at following image can you tell if the ball in the game of pong going left or right?\n",
    "    <img src='assets/pong.png' width=50% /> <br>\n",
    "    Thus we stack 4 frames to get context. If it's going left or right.\n",
    "\n",
    "3) Stack CNN layers.\n",
    "\n",
    "The above presented architecture is just an example. The complexity of your network depends on the Environment.\n",
    "\n",
    "Different environment have different scoring system. To avoid having different architectures to play different games we often **clip rewards**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Obx8qWmVsLWv",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward):\n",
    "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
    "        return np.sign(reward)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N_TWgMYIsLW0",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class LazyFrames(object):\n",
    "    def __init__(self, frames):\n",
    "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
    "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
    "        buffers.\n",
    "        This object should only be converted to numpy array before being passed to the model.\n",
    "        You'd not believe how complex the previous solution was.\"\"\"\n",
    "        self._frames = frames\n",
    "        self._out = None\n",
    "\n",
    "    def _force(self):\n",
    "        if self._out is None:\n",
    "            self._out = np.concatenate(self._frames, axis=2)\n",
    "            self._frames = None\n",
    "        return self._out\n",
    "\n",
    "    def __array__(self, dtype=None):\n",
    "        out = self._force()\n",
    "        if dtype is not None:\n",
    "            out = out.astype(dtype)\n",
    "        return out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._force())\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self._force()[i]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IR5oT-pjsLXB",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k):\n",
    "        \"\"\"Stack k last frames.\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        See Also\n",
    "        --------\n",
    "        baselines.common.atari_wrappers.LazyFrames\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return LazyFrames(list(self.frames))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gaXQWVitsLXI",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # careful! This undoes the memory optimization, use\n",
    "        # with smaller replay buffers only.\n",
    "        return np.array(observation).astype(np.float32) / 255.0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cJBhnmbGsLXO",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class PreProcessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, cfg):\n",
    "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = cfg.frame_width\n",
    "        self.height = cfg.frame_height\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255,\n",
    "            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cYsqRBrFsLXU",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Image shape to num_channels x weight x height\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.swapaxes(observation, 2, 0)\n",
    "\n",
    "\n",
    "def convert_pytorch(env):\n",
    "    return ImageToPyTorch(env)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vhc43cCysLXd",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def wrap_env(env, cfg,clip_rewards=True, frame_stack=True, scale=True):\n",
    "    \"\"\"Configure environment for DeepMind-style Atari.\n",
    "    \"\"\"\n",
    "    env = PreProcessFrame(env, cfg)\n",
    "    if scale:\n",
    "        env = ScaledFloatFrame(env)\n",
    "    if clip_rewards:\n",
    "        env = ClipRewardEnv(env)\n",
    "    if frame_stack:\n",
    "        env = FrameStack(env, cfg.frame_number)\n",
    "    return env"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Km43lgj1sLXi",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class Config:\n",
    "    \"\"\"Config for environment\"\"\"\n",
    "    def __init__(self):\n",
    "        self.frame_number = 4\n",
    "        self.frame_width = 84\n",
    "        self.frame_height = 84\n",
    "        \n",
    "    def __setitem__(self, key, value):\n",
    "        return setattr(self, key, value)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "YfGynEJzsLXn",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "env_id = \"PongNoFrameskip-v4\"\n",
    "config = Config()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "awGkWFyqsLXt",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class Environment:\n",
    "    def __init__(self, env_id, config):\n",
    "        self.env = gym.make(env_id)\n",
    "        self.env = wrap_env(self.env, config)\n",
    "        self.env = convert_pytorch(self.env)\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        self.spec = self.env.spec\n",
    "        self.states = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = self.env.reset()\n",
    "        return self.states\n",
    "\n",
    "    def step(self, a):\n",
    "        self.states, r, done, info = self.env.step(a)\n",
    "        return self.states, r, done, info\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NnknBRQmsLX1",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0))\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "IBdyzVSxsLX5",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "env = Environment(env_id=env_id, config=config)\n",
    "current_model = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "target_model  = CnnDQN(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "if USE_CUDA:\n",
    "    current_model = current_model.cuda()\n",
    "    target_model  = target_model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(current_model.parameters(), lr=0.0001)\n",
    "\n",
    "replay_initial = 1000\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "\n",
    "update_target(current_model, target_model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GV52ttfNsLYD",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MJw09nY7sLYK",
    "colab_type": "code",
    "outputId": "96c08e78-cf25-48d5-fc58-1349aa364746",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "num_frames = 100000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = current_model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        # TODO:- Based on Td_loss functions you have worked with write or\n",
    "        # choose a function to reuse here and justify your choice\n",
    "        loss = \n",
    "        losses.append(loss.data)\n",
    "        \n",
    "    if frame_idx % 10000 == 0:\n",
    "        plot(frame_idx, all_rewards, losses)\n",
    "        \n",
    "    if frame_idx % 1000 == 0:\n",
    "        update_target(current_model, target_model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WyTyFYlcsLYT",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}