{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 8\n",
    "\n",
    "### What is A2C?\n",
    "\n",
    "Since the beginning of this course, we’ve studied two different reinforcement learning methods:\n",
    "\n",
    "* **Value based methods** (Q-learning, Deep Q-learning): where we learn a value function that will map each state action pair to a value.\n",
    "* **Policy based methods** (REINFORCE with Policy Gradients): where we directly optimize the policy without using a value function.\n",
    "\n",
    "But both of these methods have big drawbacks. That’s why, today, we’ll study a new type of Reinforcement Learning method which we can call a “hybrid method”: Actor Critic. We’ll using two neural networks:\n",
    "\n",
    "* **Actor**: Controls how our agent behaves (policy-based)\n",
    "* **Critic**:  Measures how good the state is (value-based)\n",
    "\n",
    "The Actor Critic model is a better score function. Instead of waiting until the end of the episode as we do in Monte Carlo REINFORCE, we make an update at each step (TD Learning).\n",
    "\n",
    "\n",
    "At the beginning, you don’t know how to play, so you try some action randomly. The Critic observes your action and provides feedback.\n",
    "Learning from this feedback, you’ll update your policy and be better at playing that game.\n",
    "\n",
    "On the other hand, your friend (Critic) will also update their own way to provide feedback so it can be better next time.\n",
    "As we can see, the idea of Actor Critic is to have two function approximator, the policy (actor) and the value function (critic). We estimate both with neural networks.\n",
    "\n",
    "Because we have two models (Actor and Critic) that must be trained, it means that we have two set of weights that must be optimized separately.\n",
    "\n",
    "### Submitting the code and experiment runs\n",
    "In order to turn in your code and report, create 3 folders that contains\n",
    "the following:\n",
    "* Cartpole\n",
    "    * A2C and PPO code using cartpole.\n",
    "    * Plots \n",
    "* LunarLander\n",
    "    * A2C and PPO code using LunarLander-v2 env\n",
    "    * Plots\n",
    "* Report\n",
    "    * Report\n",
    "\n",
    "A scientific report explaining\n",
    " * Difference between A2C and PPO?\n",
    " * Try solving LunarLander-v2 env. using already implemented code for cartpole Set N_TRIALS = 1 and REWARD_THRESHOLD = 100. You are only allowed to change MAX_EPISODES.\n",
    " * Compare the performance of A2C and PPO in different envs\n",
    " * If any difference please explain why?\n",
    " * What can be potential improvements?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "env_id = 'LunarLander-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-789a6f184c92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UA/1e master/DAI/Opdrachten/RL_Lab/venv2/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UA/1e master/DAI/Opdrachten/RL_Lab/venv2/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UA/1e master/DAI/Opdrachten/RL_Lab/venv2/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UA/1e master/DAI/Opdrachten/RL_Lab/venv2/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gym.envs.box2d' has no attribute 'LunarLander'"
     ],
     "ename": "AttributeError",
     "evalue": "module 'gym.envs.box2d' has no attribute 'LunarLander'",
     "output_type": "error"
    }
   ],
   "source": [
    "train_env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "train_env.seed(SEED);\n",
    "eval_env.seed(int(SEED/2))\n",
    "np.random.seed(SEED);\n",
    "torch.manual_seed(SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.5):\n",
    "        \"\"\"\n",
    "        This is a basic Artificial Neural Network that can be used for both the Actor and the Critic.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "        \n",
    "        hidden_dim: int\n",
    "        \n",
    "        output_dim: int\n",
    "        \n",
    "        dropout: float\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Prediction of the Neural Network given an input x (in RL, x is a state).\n",
    "        The Network uses a dropout layer (to help generalize), and the ReLU activation function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: tensor\n",
    "            input, i.e. state\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        x: tensor\n",
    "            the network's prediction (output) for this input.\n",
    "        \"\"\"\n",
    "        # TODO: write forward pass for NN\n",
    "        # Hint: it should have layers, dropout, relu.\n",
    "        \n",
    "        x = self.fc_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, actor, critic):\n",
    "        \"\"\"\n",
    "        This is a joint model, with two ANNs within.\n",
    "        Parameters\n",
    "        ----------\n",
    "        actor: BaseModel instance\n",
    "        \n",
    "        critic: BaseModel instance\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\" \n",
    "        The output of the ActorCritic model is the concatenation of the actor and critic's outputs.\n",
    "        Since the actor is a policy, we convert the output into probabilities using a softmax function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        state: tensor\n",
    "            model input, i.e. state the agent is in\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        action_pred, value_pred: tensor, tensor\n",
    "            the network's prediction (output) for this input.\n",
    "        \"\"\"\n",
    "\n",
    "        action_pred = self.actor(state)\n",
    "        value_pred = self.critic(state)\n",
    "        \n",
    "        # TODO: convert actions to probabilities using softmax\n",
    "        action_pred = F.softmax(action_pred)\n",
    "        \n",
    "        return action_pred, value_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Reinitialise agent again if you change any hyperparameter. Don't retrain same agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "INPUT_DIM = train_env.observation_space.shape[0]\n",
    "HIDDEN_DIM = 128\n",
    "\n",
    "# TODO: What should be the output dimention for actor and critic.\n",
    "# HINT: actor controls policy and critic outputs only values\n",
    "print(train_env.action_space)\n",
    "OUTPUT_DIM_ACTOR = train_env.action_space.n\n",
    "OUTPUT_DIM_CRITIC = 1\n",
    "actor = BaseModel(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM_ACTOR)\n",
    "critic = BaseModel(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM_CRITIC)\n",
    "\n",
    "agent = ActorCritic(actor, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \"\"\" Initializes the ANNs weights with a relevant distribution. \"\"\"\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "agent.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "\n",
    "optimizer = optim.Adam(agent.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train(env, agent, optimizer, discount_factor):\n",
    "    \"\"\"\n",
    "    Performs a single training step over an episode.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    agent: ActorCritic\n",
    "    \n",
    "    optimizer: PyTorch optimizer\n",
    "    \n",
    "    discount_factor: float\n",
    "        discount gamma\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    policy_loss: float \n",
    "        loss of the policy (actor)\n",
    "    \n",
    "    value_loss: float\n",
    "        loss of the value function approximator (critic)\n",
    "    \n",
    "    episode_reward: float\n",
    "        reward for this episode\n",
    "    \"\"\"\n",
    "    agent.train()\n",
    "    \n",
    "    log_prob_actions = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # TODO: get action\n",
    "        action_prob, value_pred = agent(state)\n",
    "         \n",
    "        dist = distributions.Categorical(action_prob)\n",
    "\n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        \n",
    "        if env_id == 'Pendulum-v0':\n",
    "            state, reward, done, _ = env.step(action)\n",
    "        else:\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        values.append(value_pred)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        episode_reward += reward\n",
    "    \n",
    "    log_prob_actions = torch.cat(log_prob_actions)\n",
    "    values = torch.cat(values).squeeze(-1)\n",
    "    \n",
    "    returns = calculate_returns(rewards, discount_factor)\n",
    "    advantages = calculate_advantages(returns, values)\n",
    "    \n",
    "    policy_loss, value_loss = update_policy(advantages, log_prob_actions, returns, values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    return policy_loss.item(), value_loss.item(), episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, discount_factor, normalize = True):\n",
    "    \"\"\"\n",
    "    Function to calculate rewards in time step order and normalize them.\n",
    "    Normalizing stabilizes the results.\n",
    "    Parameters\n",
    "    ----------\n",
    "    rewards: list of floats\n",
    "    \n",
    "    discount_factor: float\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    returns: tensor\n",
    "        tensor of returns G_t in time-step order\n",
    "    \"\"\"\n",
    "    # TODO: calculate future rewards\n",
    "    rewards.reverse()\n",
    "    returns = []\n",
    "    prev = 0\n",
    "    for i in rewards:\n",
    "        i = i+prev*discount_factor\n",
    "        prev = i \n",
    "        returns.append(i)\n",
    "    returns.reverse()\n",
    "    returns = torch.tensor(returns, dtype=torch.float64)\n",
    "    \n",
    "    if normalize:\n",
    "        \n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "        \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_advantages(returns, values, normalize = True):\n",
    "    \"\"\"\n",
    "    Computes the advantage for all actions. \n",
    "    Reminder: the Advantage function for an action a is A(s,a) = Q(s,a) - V(s)\n",
    "    Normalizing stabilizes the results.\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns: tensor\n",
    "        Returns G_t during an episode\n",
    "    \n",
    "    values: tensor\n",
    "        Value estimates V(s_t)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    advantages: tensor\n",
    "    \"\"\"\n",
    "    # TODO: calculate advantage\n",
    "    advantages = returns - values\n",
    "    \n",
    "    # TODO: write code to normalize the values\n",
    "    if normalize:\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "        \n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def update_policy(advantages, log_prob_actions, returns, values):\n",
    "    \"\"\"\n",
    "    Function to update your policy based on your actor and critic loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    advantages: tensor\n",
    "    \n",
    "    log_prob_actions: tensor\n",
    "    \n",
    "    returns: tensor\n",
    "    \n",
    "    values: tensor\n",
    "    \n",
    "    optimizer: adam instance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    advantages = advantages.detach()\n",
    "    returns = returns.detach()\n",
    "    \n",
    "    # TODO: calculate policy loss based on advantages and log_prob_actions.\n",
    "    policy_loss = F.mse_loss(advantages , log_prob_actions)\n",
    "    \n",
    "    # TODO: calculate value loss based on Mean Absolute Error\n",
    "    value_loss = F.l1_loss(returns, values)\n",
    "        \n",
    "    \n",
    "    return policy_loss, value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(env, agent, vis=False):\n",
    "    \"\"\"\n",
    "    Function to evaluate your agent's performance.\n",
    "    \"\"\"\n",
    "    agent.eval()\n",
    "    \n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    if vis: env.render()\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            action_prob, _ = agent(state)\n",
    "                \n",
    "        action = torch.argmax(action_prob, dim = -1)\n",
    "                \n",
    "        if env_id == 'Pendulum-v0':\n",
    "            state, reward, done, _ = env.step(action)\n",
    "        else:\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        episode_reward += reward\n",
    "        \n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot(frame_idx, train_rewards, policy_loss, value_loss):\n",
    "    \"\"\"\n",
    "    Plots the running reward and losses.\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame_idx: int\n",
    "        frame id\n",
    "    rewards: int\n",
    "        accumulated reward\n",
    "    losses: int\n",
    "        loss\n",
    "    \"\"\"\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(train_rewards[-10:])))\n",
    "    plt.plot(train_rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('policy loss')\n",
    "    plt.plot(policy_loss)\n",
    "    plt.subplot(133)\n",
    "    plt.title('value loss')\n",
    "    plt.plot(value_loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_EPISODES = 500\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 475\n",
    "\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "train_rewards = []\n",
    "test_rewards = []\n",
    "\n",
    "for episode in range(1, MAX_EPISODES+1):\n",
    "    \n",
    "    policy_loss, value_loss, train_reward = train(train_env, agent, optimizer, DISCOUNT_FACTOR)\n",
    "    test_reward = evaluate(eval_env, agent, False)\n",
    "    \n",
    "    train_rewards.append(train_reward)\n",
    "    test_rewards.append(test_reward)\n",
    "    policy_losses.append(policy_loss)\n",
    "    value_losses.append(value_loss)\n",
    "    \n",
    "    mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\n",
    "    mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\n",
    "    \n",
    "    plot(episode, train_rewards, policy_losses, value_losses)\n",
    "\n",
    "    if mean_test_rewards >= REWARD_THRESHOLD:\n",
    "        \n",
    "        print(f'Reached reward threshold in {episode} episodes')\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_evaluate():\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(test_rewards, label='Test Reward')\n",
    "    plt.plot(train_rewards, label='Train Reward')\n",
    "    plt.xlabel('Episode', fontsize=20)\n",
    "    plt.ylabel('Reward', fontsize=20)\n",
    "    plt.hlines(REWARD_THRESHOLD, 0, len(test_rewards), color='r')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plot_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL algorithms have many moving parts that are hard to debug, and they require substantial effort in tuning in order to get good results.\n",
    "PPO strikes a balance between ease of implementation, sample complexity, and ease of tuning, trying to compute an update at each step that minimizes the cost function while ensuring the deviation from the previous policy is relatively small.\n",
    "The central idea for Proximal Policy Optimization is to avoid having too large policy update\n",
    "\n",
    "PPO uses an adaptive KL penalty to control the change of the policy at each iteration. It uses an objective function not typically found in other algorithms.\n",
    "\n",
    "Here we implement PPO agent in A2C style. Which means it follows same actor critic implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinitialise your agents\n",
    "##### TODO: Reuse A2C methods to validate if PPO is running correctly with cartpole-env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "agent = ActorCritic(actor, critic)\n",
    "agent.apply(init_weights)\n",
    "optimizer = optim.Adam(agent.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def update_policy_ppo(agent, states, actions, log_prob_actions, advantages, returns, optimizer, ppo_steps, ppo_clip):\n",
    "    \n",
    "    total_policy_loss = 0 \n",
    "    total_value_loss = 0\n",
    "    \n",
    "    advantages = advantages.detach()\n",
    "    log_prob_actions = log_prob_actions.detach()\n",
    "    actions = actions.detach()\n",
    "    \n",
    "    for _ in range(ppo_steps):\n",
    "                \n",
    "        #get new log prob of actions for all input states\n",
    "        action_prob, value_pred = agent(states)\n",
    "        value_pred = value_pred.squeeze(-1)\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        \n",
    "        #new log prob using old actions\n",
    "        new_log_prob_actions = dist.log_prob(actions)\n",
    "        \n",
    "        policy_ratio = (new_log_prob_actions - log_prob_actions).exp()\n",
    "                \n",
    "        policy_loss_1 = policy_ratio * advantages\n",
    "        policy_loss_2 = torch.clamp(policy_ratio, min = 1.0 - ppo_clip, max = 1.0 + ppo_clip) * advantages\n",
    "        \n",
    "        policy_loss = - torch.min(policy_loss_1, policy_loss_2).sum()\n",
    "        \n",
    "        value_loss = F.smooth_l1_loss(returns, value_pred).sum()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        policy_loss.backward()\n",
    "        value_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "        total_policy_loss += policy_loss.item()\n",
    "        total_value_loss += value_loss.item()\n",
    "    \n",
    "    return total_policy_loss / ppo_steps, total_value_loss / ppo_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def train_ppo(env, agent, optimizer, discount_factor, ppo_steps, ppo_clip):\n",
    "        \n",
    "    agent.train()\n",
    "        \n",
    "    states = []\n",
    "    actions = []\n",
    "    log_prob_actions = []\n",
    "    values = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "        #append state here, not after we get the next state from env.step()\n",
    "        states.append(state)\n",
    "        \n",
    "        action_prob, value_pred = agent(state)\n",
    "                \n",
    "        dist = distributions.Categorical(action_prob)\n",
    "        \n",
    "        action = dist.sample()\n",
    "        \n",
    "        log_prob_action = dist.log_prob(action)\n",
    "        \n",
    "        if env_id == 'Pendulum-v0':\n",
    "            state, reward, done, _ = env.step(action)\n",
    "        else:\n",
    "            state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        actions.append(action)\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        values.append(value_pred)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        episode_reward += reward\n",
    "    \n",
    "    states = torch.cat(states)\n",
    "    actions = torch.cat(actions)    \n",
    "    log_prob_actions = torch.cat(log_prob_actions)\n",
    "    values = torch.cat(values).squeeze(-1)\n",
    "    \n",
    "    returns = calculate_returns(rewards, discount_factor)\n",
    "    advantages = calculate_advantages(returns, values)\n",
    "    \n",
    "    policy_loss, value_loss = update_policy_ppo(agent, states, actions, log_prob_actions, advantages, returns, optimizer, ppo_steps, ppo_clip)\n",
    "\n",
    "    return policy_loss, value_loss, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_EPISODES = 500\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "N_TRIALS = 25\n",
    "REWARD_THRESHOLD = 475\n",
    "PPO_STEPS = 5\n",
    "PPO_CLIP = 0.2\n",
    "\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "train_rewards = []\n",
    "test_rewards = []\n",
    "\n",
    "for episode in range(1, MAX_EPISODES+1):\n",
    "    \n",
    "    policy_loss, value_loss, train_reward = train_ppo(train_env, agent, optimizer, DISCOUNT_FACTOR, PPO_STEPS, PPO_CLIP)\n",
    "    \n",
    "    test_reward = evaluate(eval_env, agent)\n",
    "    \n",
    "    train_rewards.append(train_reward)\n",
    "    test_rewards.append(test_reward)\n",
    "    policy_losses.append(policy_loss)\n",
    "    value_losses.append(value_loss)\n",
    "\n",
    "    \n",
    "    mean_train_rewards = np.mean(train_rewards[-N_TRIALS:])\n",
    "    mean_test_rewards = np.mean(test_rewards[-N_TRIALS:])\n",
    "    \n",
    "    plot(episode, train_rewards, policy_losses, value_losses)\n",
    "    \n",
    "    if mean_test_rewards >= REWARD_THRESHOLD:\n",
    "        \n",
    "        print(f'Reached reward threshold in {episode} episodes')\n",
    "        \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}